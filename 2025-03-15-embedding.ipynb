{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"i love nlp\", \" i teach gen ai \", \"i am working with euron\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = list(set(\" \".join(corpus).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nlp', 'euron', 'gen', 'working', 'teach', 'i', 'ai', 'love', 'with', 'am']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'nlp')\n",
      "(1, 'euron')\n",
      "(2, 'gen')\n",
      "(3, 'working')\n",
      "(4, 'teach')\n",
      "(5, 'i')\n",
      "(6, 'ai')\n",
      "(7, 'love')\n",
      "(8, 'with')\n",
      "(9, 'am')\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(unique_words):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word:i for i, word in enumerate(unique_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nlp', 'euron', 'gen', 'working', 'teach', 'i', 'ai', 'love', 'with', 'am']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nlp': 0,\n",
       " 'euron': 1,\n",
       " 'gen': 2,\n",
       " 'working': 3,\n",
       " 'teach': 4,\n",
       " 'i': 5,\n",
       " 'ai': 6,\n",
       " 'love': 7,\n",
       " 'with': 8,\n",
       " 'am': 9}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love nlp\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " i teach gen ai \n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "i am working with euron\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "one_hot_vector = []\n",
    "for sentence in corpus:\n",
    "    print(sentence)\n",
    "    sentence_vector = []\n",
    "    for word in sentence.split():\n",
    "        vector = [0] * len(unique_words)    # sparse vector -> vector with 0\n",
    "        print(vector)\n",
    "        vector[word_to_index[word]] = 1\n",
    "        #print(word_to_index[word])\n",
    "        #print(vector[word_to_index[word]])\n",
    "        sentence_vector.append(vector)\n",
    "    one_hot_vector.append(sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]],\n",
       " [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantates of One Hot Encoding - \n",
    "1. Lot of sparse data (lot of zero's)\n",
    "2. For small sentences also dimentional matrix could be huge, as number of columns would depend on unique set of words\n",
    "3. No contextual relation between different matrices generated\n",
    "\n",
    "One Hot Encoding - \n",
    "One Hot Encoding is called so because it represents categorical variables as binary vectors. Each category is converted into a vector where one element is \"hot\" (1) and all others are \"cold\" (0). This way, each category is uniquely identified without implying any ordinal relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW) \n",
    "Frequency based encoding technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=None, lowercase=False, vocabulary=unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"i love nlp nlp\", \"love i nlp nlp\", \" i teach gen ai gen ai gen ai\", \"i am working with euron\"] \n",
    "# Note: \"i love nlp\" is same as \"love i nlp\" -> doesn't differentiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love nlp nlp',\n",
       " 'love i nlp nlp',\n",
       " ' i teach gen ai gen ai gen ai',\n",
       " 'i am working with euron']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [2, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 3, 0, 1, 0, 3, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nlp', 'euron', 'gen', 'working', 'teach', 'i', 'ai', 'love',\n",
       "       'with', 'am'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all unique features it has considered\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF - IDF \n",
    "Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF = Number of times word appeared in a document / Total number of words in document \n",
    "# IDF = log (Total number of document / Number of document containing word) \n",
    "\n",
    "# TF-IDF = TF * IDF -> Represents numeric format of a word\n",
    "\n",
    "# Repeatative words has less impact than the unique words.. Creates Less Sparse\n",
    "# if frequency is more , impact is less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pappu nach nahi sakta', 'pappu loves dancing', 'pappu stops dancing']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus = [\"i love nlp nlp\", \" i teach gen ai gen ai gen ai\", \"i am working with euron\"]\n",
    "#corpus = [\"i love nlp\", \"i love love nlp\", \" i teach gen ai gen ai gen ai\", \"i am working with euron\", \"unique set\"]\n",
    "\n",
    "corpus = [\"pappu nach nahi sakta\", \"pappu loves dancing\", \"pappu stops dancing\"]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_tf_idf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vect_tf_idf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.54645401, 0.54645401, 0.32274454,\n",
       "        0.54645401, 0.        ],\n",
       "       [0.54783215, 0.72033345, 0.        , 0.        , 0.42544054,\n",
       "        0.        , 0.        ],\n",
       "       [0.54783215, 0.        , 0.        , 0.        , 0.42544054,\n",
       "        0.        , 0.72033345]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage and Disadvatage of TF-IDF\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) has several advantages and disadvantages:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Simplicity: Easy to understand and implement.\n",
    "Effectiveness: Works well for text classification and information retrieval.\n",
    "Relevance: Highlights important words in documents by reducing the weight of common terms.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Sparsity: Can create high-dimensional sparse vectors, which may lead to inefficiencies.\n",
    "Context Ignorance: Does not consider word order or context, potentially losing semantic meaning.\n",
    "Static: Does not adapt to changes in language or context over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the following embedding techniques are discussed - \n",
    "1. One Hot Encoding [Worst Technique]\n",
    "2. Bag of Words\n",
    "3. TF-IDF\n",
    "\n",
    "Problem with all above mentioned techniques is -\n",
    "- Unable to understand the ordering of the words\n",
    "- Unable to understand semantic and syntactic (ordering) of words to comprehend its meaning\n",
    "- Unable to establish relationship or meaning between words and its grammer\n",
    "\n",
    "So, the next technique to discuss is Word2Vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
